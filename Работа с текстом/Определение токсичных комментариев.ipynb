{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Введение\" data-toc-modified-id=\"Введение-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Введение</a></span></li><li><span><a href=\"#Предобработка\" data-toc-modified-id=\"Предобработка-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Предобработка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Предобработка-текста\" data-toc-modified-id=\"Предобработка-текста-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Предобработка текста</a></span><ul class=\"toc-item\"><li><span><a href=\"#Лемматизируем-текст\" data-toc-modified-id=\"Лемматизируем-текст-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Лемматизируем текст</a></span></li><li><span><a href=\"#Уберем-неинформативные-символы-из-текста\" data-toc-modified-id=\"Уберем-неинформативные-символы-из-текста-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Уберем неинформативные символы из текста</a></span></li><li><span><a href=\"#Закодируем-чистый-текс-в-векторы\" data-toc-modified-id=\"Закодируем-чистый-текс-в-векторы-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Закодируем чистый текс в векторы</a></span></li></ul></li><li><span><a href=\"#Выводы:\" data-toc-modified-id=\"Выводы:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Выводы:</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модель-логистической-регрессии\" data-toc-modified-id=\"Модель-логистической-регрессии-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Модель логистической регрессии</a></span></li><li><span><a href=\"#Модель-случайного-леса\" data-toc-modified-id=\"Модель-случайного-леса-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Модель случайного леса</a></span></li><li><span><a href=\"#Модель-CatBoost\" data-toc-modified-id=\"Модель-CatBoost-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Модель CatBoost</a></span></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Тестирование</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "    Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "    \n",
    "    В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "    Наша задача - построить модель со значением метрики качества F1 не меньше 0.75. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\krulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\krulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def lemmatize(text):\n",
    "#     word_list = nltk.word_tokenize(text)\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "   \n",
    "#     return ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['lemm_text'] = df['text'].progress_apply(lemmatize)\n",
    "# df = df.drop(['text'], axis=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уберем неинформативные символы из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для отчистки текста и создадим новый столбец\n",
    "\n",
    "def clear_text(text):\n",
    "    text_1 = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    final_text = \" \".join(text_1.split())\n",
    "    return final_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                         clear_text\n",
       "0      0  Explanation Why the edits made under my userna...\n",
       "1      0  D aww He matches this background colour I m se...\n",
       "2      0  Hey man I m really not trying to edit war It s...\n",
       "3      0  More I can t make any real suggestions on impr...\n",
       "4      0  You sir are my hero Any chance you remember wh..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clear_text'] = df['text'].apply(clear_text)\n",
    "df = df.drop(['text'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                         clear_text\n",
       "0      0  explanation why the edits made under my userna...\n",
       "1      0  d aww he matches this background colour i m se...\n",
       "2      0  hey man i m really not trying to edit war it s...\n",
       "3      0  more i can t make any real suggestions on impr...\n",
       "4      0  you sir are my hero any chance you remember wh..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clear_text'] = df['clear_text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Закодируем чистый текс в векторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127433,)\n",
      "(31859,)\n",
      "(127433,)\n",
      "(31859,)\n"
     ]
    }
   ],
   "source": [
    "# Разделим выборку на тренировочную и тестовую \n",
    "features = df['clear_text']\n",
    "target = df['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=42, test_size=0.2, stratify=target)\n",
    "\n",
    "# проверим по форме\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = features_train.values\n",
    "corpus_test = features_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Сформируем список стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf = TfidfVectorizer(stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем векторные признаки для обучения\n",
    "features_train = count_tf.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем векторные признаки для теста\n",
    "features_test = count_tf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127433, 147534)\n",
      "(31859, 147534)\n"
     ]
    }
   ],
   "source": [
    "# проверим по форме\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "    Мы убрали из текста ненужные символы, убрали стоп слова и сформировали векторные признаки. Mожем переходить к моделированию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "250 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.69126317        nan 0.69128381        nan 0.69120345\n",
      "        nan 0.69137101        nan 0.69210646        nan 0.69231538\n",
      "        nan 0.6918796         nan 0.69190205        nan 0.69094294\n",
      "        nan 0.69124838        nan 0.68993855        nan 0.68930456\n",
      "        nan 0.68844527        nan 0.68834204        nan 0.6886305\n",
      "        nan 0.69092725        nan 0.69513692        nan 0.70012433\n",
      "        nan 0.7068779         nan 0.71447688        nan 0.72303511\n",
      "        nan 0.73010529        nan 0.73808885        nan 0.74506676\n",
      "        nan 0.75038866        nan 0.75591789        nan 0.76048778\n",
      "        nan 0.76402381        nan 0.76666835        nan 0.76929076\n",
      "        nan 0.76900486        nan 0.76807325        nan 0.76665235\n",
      "        nan 0.76595486        nan 0.76395988        nan 0.76038661\n",
      "        nan 0.75768733        nan 0.7563121         nan 0.75315537\n",
      "        nan 0.75313307        nan 0.75057207        nan 0.75120953\n",
      "        nan 0.74775842        nan 0.75160638        nan 0.74492182\n",
      "        nan 0.74301944        nan 0.74141512        nan 0.74128529\n",
      "        nan 0.74269366        nan 0.7410821 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\krulo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight='balanced',\n",
       "                                          random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([1.00000000e-04, 1.45634848e-04, 2.12095089e-04, 3.08884360e-04,\n",
       "       4.49843267e-04, 6.55128557e-04, 9.54095476e-04, 1.38949549e-03,\n",
       "       2.02358965e-03, 2.94705170e-03, 4.29193426e-03, 6.25055193e-03,\n",
       "       9.10298178e-03, 1.32571137e-02, 1.93069773e-02, 2.811...\n",
       "       3.72759372e+00, 5.42867544e+00, 7.90604321e+00, 1.15139540e+01,\n",
       "       1.67683294e+01, 2.44205309e+01, 3.55648031e+01, 5.17947468e+01,\n",
       "       7.54312006e+01, 1.09854114e+02, 1.59985872e+02, 2.32995181e+02,\n",
       "       3.39322177e+02, 4.94171336e+02, 7.19685673e+02, 1.04811313e+03,\n",
       "       1.52641797e+03, 2.22299648e+03, 3.23745754e+03, 4.71486636e+03,\n",
       "       6.86648845e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LogisticRegression(random_state=1, class_weight = 'balanced')\n",
    "\n",
    "params = {'C': np.logspace(-4, 4, 50),\n",
    "          'penalty': ['l1', 'l2']}\n",
    "\n",
    "grid_CV = GridSearchCV(estimator=model_lr, \n",
    "                         param_grid=params, \n",
    "                         scoring = 'f1',\n",
    "                         cv=5,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "grid_CV.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5.428675439323859, 'penalty': 'l2'}\n",
      "\n",
      "Оценка лучшей модели логистической регрессии на кросс-валидации 0.7692907645331523\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_CV.best_params_\n",
    "print(best_params)\n",
    "print()\n",
    "best_score = grid_CV.best_score_\n",
    "print('Оценка лучшей модели логистической регрессии на кросс-валидации', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "   class_weight = 'balanced' дает улучшение в третьем знаке после запятой)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# params = {'max_depth' : [4,6,8,10],\n",
    "#           'n_estimators' : range(10,20,2)}\n",
    "\n",
    "# grid_CV_rf = GridSearchCV(estimator=model_rf, \n",
    "#                          param_grid=params, \n",
    "#                          scoring = 'f1',\n",
    "#                          cv=3,\n",
    "#                          n_jobs=-1)\n",
    "\n",
    "# grid_CV_rf.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = grid_CV_rf.best_params_\n",
    "# print(best_params)\n",
    "# print()\n",
    "# best_score = grid_CV_rf.best_score_\n",
    "# print('Оценка лучшей модели случайного леса на кросс-валидации', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим валидационные выборки для катбуста\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = CatBoostClassifier(iterations = 1000, learning_rate = 0.03)\n",
    "\n",
    "model_gb.fit(features_train, target_train, eval_set=(features_valid, target_valid), verbose=False, use_best_model = True)\n",
    "\n",
    "print('Params')\n",
    "print(model_gb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred = model_gb.predict(features_test)\n",
    "print('F1 на модели градиентного бустинга', f1_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на модели логистической регрессии 0.7675457558726041\n"
     ]
    }
   ],
   "source": [
    "# Лучшую оценку на тренировочной выборке получила модель логистической регрессии\n",
    "\n",
    "preds = grid_CV.predict(features_test)\n",
    "print('F1 на модели логистической регрессии', f1_score(target_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "    В этом проекте нам была поставлена задача определения токсичности твитов. Для работы у нас было более 160 тысяч размеченных сообщений. \n",
    "   \n",
    "    В ходе подготовки данных к обучению мы \n",
    "        1. лемматизировали (нет) данные\n",
    "        2. избавились от слов, которые не несут содержательного значения (стоп-слова)\n",
    "        3. перевели сообщения в векторный формат\n",
    "       \n",
    "    На этапе обучения мы рассмотрели три модели: логистическую регрессию, случайный лес и КатБуст. Лучший результат показала модель логистической регрессии. На тестовой выборке мы молучили значение метрики F1 на уровне 76%, что превышало необходимое значение в техническом задании.\n",
    "    \n",
    "    Достижение необходимой точности предсказания модели стало возможным благодаря перебору гиперпараметров моделей с помощью кросс-валидации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3797,
    "start_time": "2023-04-09T07:30:28.965Z"
   },
   {
    "duration": 6439,
    "start_time": "2023-04-09T07:31:14.250Z"
   },
   {
    "duration": 1199,
    "start_time": "2023-04-09T07:32:08.733Z"
   },
   {
    "duration": 1249,
    "start_time": "2023-04-09T07:32:25.226Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-09T07:43:29.842Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-09T07:45:53.618Z"
   },
   {
    "duration": 1330,
    "start_time": "2023-04-09T07:45:53.628Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-09T07:45:54.965Z"
   },
   {
    "duration": 3919,
    "start_time": "2023-04-09T07:45:54.979Z"
   },
   {
    "duration": 3725,
    "start_time": "2023-04-09T07:46:49.688Z"
   },
   {
    "duration": 3821,
    "start_time": "2023-04-09T07:46:59.220Z"
   },
   {
    "duration": 3833,
    "start_time": "2023-04-09T07:47:35.101Z"
   },
   {
    "duration": 2748,
    "start_time": "2023-04-09T07:48:13.341Z"
   },
   {
    "duration": 3695,
    "start_time": "2023-04-09T07:48:22.544Z"
   },
   {
    "duration": 1777,
    "start_time": "2023-04-09T07:48:50.532Z"
   },
   {
    "duration": 1850,
    "start_time": "2023-04-09T07:49:11.223Z"
   },
   {
    "duration": 2177,
    "start_time": "2023-04-09T07:49:31.552Z"
   },
   {
    "duration": 1748,
    "start_time": "2023-04-09T07:49:57.062Z"
   },
   {
    "duration": 1214,
    "start_time": "2023-04-09T07:49:58.826Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-09T07:50:00.045Z"
   },
   {
    "duration": 3081,
    "start_time": "2023-04-09T07:50:00.056Z"
   },
   {
    "duration": 8507,
    "start_time": "2023-04-09T07:57:07.206Z"
   },
   {
    "duration": 7422,
    "start_time": "2023-04-09T07:57:15.715Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-09T07:57:23.139Z"
   },
   {
    "duration": 4289,
    "start_time": "2023-04-09T07:57:23.151Z"
   },
   {
    "duration": 4214,
    "start_time": "2023-04-09T07:58:19.650Z"
   },
   {
    "duration": 6396,
    "start_time": "2023-04-09T07:58:23.866Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-09T07:58:30.264Z"
   },
   {
    "duration": 3654,
    "start_time": "2023-04-09T07:58:30.269Z"
   },
   {
    "duration": 5375,
    "start_time": "2023-04-09T08:03:17.051Z"
   },
   {
    "duration": 7577,
    "start_time": "2023-04-09T08:03:22.428Z"
   },
   {
    "duration": 23,
    "start_time": "2023-04-09T08:03:30.019Z"
   },
   {
    "duration": 3726,
    "start_time": "2023-04-09T08:03:30.051Z"
   },
   {
    "duration": 104,
    "start_time": "2023-04-09T08:03:48.558Z"
   },
   {
    "duration": 91,
    "start_time": "2023-04-09T11:03:59.528Z"
   },
   {
    "duration": 76,
    "start_time": "2023-04-10T06:07:21.127Z"
   },
   {
    "duration": 1594,
    "start_time": "2023-04-10T06:07:24.344Z"
   },
   {
    "duration": 3525,
    "start_time": "2023-04-10T06:07:27.686Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-10T06:10:47.472Z"
   },
   {
    "duration": 502,
    "start_time": "2023-04-10T06:11:20.469Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-10T06:11:46.009Z"
   },
   {
    "duration": 104923,
    "start_time": "2023-04-10T06:11:48.488Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
